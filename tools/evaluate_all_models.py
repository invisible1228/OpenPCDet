#!/usr/bin/env python3
import os
import subprocess
import argparse
import sys

def extract_eval_summary(output_text):
    """æå–è¯„ä¼°ç»“æœæ‘˜è¦"""
    lines = output_text.split('\n')
    summary_lines = []
    
    for line in lines:
        # å¯»æ‰¾åŒ…å«mAPæˆ–è¯„ä¼°ç»“æœçš„è¡Œ
        if any(keyword in line.lower() for keyword in ['map', 'precision', 'recall', 'easy', 'moderate', 'hard']):
            summary_lines.append(line)
    
    return '\n'.join(summary_lines) if summary_lines else output_text

def run_evaluation(cfg_file, ckpt_path, extra_tag, output_dir):
    """è¿è¡Œå•ä¸ªæ¨¡å‹çš„è¯„ä¼°"""
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(cfg_file):
        print(f"âŒ Config file not found: {cfg_file}")
        return None
        
    if not os.path.exists(ckpt_path):
        print(f"âŒ Checkpoint not found: {ckpt_path}")
        return None
    
    # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨å¹¶ä¸”ä½¿ç”¨ç»å¯¹è·¯å¾„
    output_dir = os.path.abspath(output_dir)
    os.makedirs(output_dir, exist_ok=True)
    
    # æµ‹è¯•æ˜¯å¦å¯ä»¥å†™å…¥æ–‡ä»¶
    test_file = os.path.join(output_dir, f"test_{extra_tag}.txt")
    try:
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        print(f"âœ… Output directory accessible: {output_dir}")
    except Exception as e:
        print(f"âŒ Cannot write to output directory {output_dir}: {e}")
        return None
    
    # åˆ‡æ¢åˆ°toolsç›®å½•
    tools_dir = "/det/OpenPCDet/tools"
    original_dir = os.getcwd()
    os.chdir(tools_dir)
    
    cmd = [
        'python', 'test.py',
        '--cfg_file', cfg_file,
        '--ckpt', ckpt_path,
        '--extra_tag', extra_tag,
        '--save_to_file'
    ]
    
    print(f"ğŸš€ Running evaluation for {extra_tag}...")
    print(f"Working directory: {os.getcwd()}")
    print(f"Command: {' '.join(cmd)}")
    
    try:
        # ä½¿ç”¨æ›´ç®€å•çš„æ–¹å¼æ‰§è¡Œå‘½ä»¤
        result = subprocess.run(
            cmd, 
            capture_output=True, 
            text=True, 
            timeout=3600  # 1å°æ—¶è¶…æ—¶
        )
        
        # æ¢å¤åŸå§‹ç›®å½•
        os.chdir(original_dir)
        
        if result.returncode == 0:
            print(f"âœ… Evaluation completed for {extra_tag}")
            
            # ä¿å­˜å®Œæ•´è¾“å‡º
            full_output_file = os.path.join(output_dir, f"eval_full_{extra_tag}.txt")
            try:
                with open(full_output_file, 'w') as f:
                    f.write(result.stdout)
                    f.write("\n--- STDERR ---\n")
                    f.write(result.stderr)
                print(f"âœ… Full output saved to: {full_output_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to save full output: {e}")
            
            # æå–å¹¶ä¿å­˜æ‘˜è¦
            summary = extract_eval_summary(result.stdout)
            summary_file = os.path.join(output_dir, f"eval_result_{extra_tag}.txt")
            try:
                with open(summary_file, 'w') as f:
                    f.write(summary)
                print(f"âœ… Summary saved to: {summary_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to save summary: {e}")
            
            return summary
        else:
            print(f"âŒ Evaluation failed for {extra_tag}")
            print(f"Return code: {result.returncode}")
            print(f"STDOUT: {result.stdout}")
            print(f"STDERR: {result.stderr}")
            
            # ä¿å­˜é”™è¯¯ä¿¡æ¯
            error_file = os.path.join(output_dir, f"eval_error_{extra_tag}.txt")
            try:
                with open(error_file, 'w') as f:
                    f.write(f"Return code: {result.returncode}\n")
                    f.write("STDOUT:\n")
                    f.write(result.stdout)
                    f.write("\nSTDERR:\n")
                    f.write(result.stderr)
                print(f"âœ… Error info saved to: {error_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to save error info: {e}")
            
            return None
            
    except subprocess.TimeoutExpired:
        print(f"â° Evaluation timed out for {extra_tag}")
        os.chdir(original_dir)
        return None
    except Exception as e:
        print(f"âŒ Exception occurred for {extra_tag}: {str(e)}")
        os.chdir(original_dir)
        return None

def test_single_model():
    """æµ‹è¯•å•ä¸ªæ¨¡å‹ï¼ˆç”¨äºè°ƒè¯•ï¼‰"""
    cfg_file = '/det/OpenPCDet/tools/cfgs/kitti_models/pointpillar.yaml'
    ckpt_path = '/det/OpenPCDet/output/kitti_models/pointpillar/baseline_pointpillar/ckpt/checkpoint_epoch_80.pth'
    extra_tag = 'baseline_pointpillar'
    
    # ä½¿ç”¨ç»å¯¹è·¯å¾„åˆ›å»ºè¾“å‡ºç›®å½•
    current_dir = os.getcwd()
    output_dir = os.path.join(current_dir, 'evaluation_results')
    print(f"ğŸ“ Using output directory: {output_dir}")
    
    os.makedirs(output_dir, exist_ok=True)
    result = run_evaluation(cfg_file, ckpt_path, extra_tag, output_dir)
    
    if result:
        print("âœ… Test successful!")
        print("Result preview:")
        print(result[:500])
    else:
        print("âŒ Test failed!")

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--test', action='store_true', help='åªæµ‹è¯•å•ä¸ªæ¨¡å‹')
    args = parser.parse_args()
    
    if args.test:
        test_single_model()
        return
    
    # åˆ›å»ºç»“æœç›®å½• - ä½¿ç”¨ç»å¯¹è·¯å¾„
    current_dir = os.getcwd()
    results_dir = os.path.join(current_dir, "evaluation_results")
    print(f"ğŸ“ Creating results directory: {results_dir}")
    os.makedirs(results_dir, exist_ok=True)
    
    # æµ‹è¯•ç›®å½•æƒé™
    try:
        test_file = os.path.join(results_dir, "test_permission.txt")
        with open(test_file, 'w') as f:
            f.write("test")
        os.remove(test_file)
        print(f"âœ… Results directory is writable")
    except Exception as e:
        print(f"âŒ Cannot write to results directory: {e}")
        return
    
    # å®šä¹‰è¦è¯„ä¼°çš„æ¨¡å‹
    models_to_eval = [
        {
            'name': 'baseline_pointpillar',
            'cfg': '/det/OpenPCDet/tools/cfgs/kitti_models/pointpillar.yaml',
            'ckpt': '/det/OpenPCDet/output/kitti_models/pointpillar/baseline_pointpillar/ckpt/checkpoint_epoch_80.pth'
        },
        {
            'name': 'pillarfocus_full',
            'cfg': '/det/OpenPCDet/tools/cfgs/kitti_models/pillarfocus_pop_rcnn.yaml',
            'ckpt': '/det/OpenPCDet/output/kitti_models/pillarfocus_pop_rcnn/pillarfocus_full/ckpt/checkpoint_epoch_80.pth'
        },
        {
            'name': 'ablation_no_dynamic',
            'cfg': '/det/OpenPCDet/tools/cfgs/kitti_models/pillarfocus_ablation_no_dynamic.yaml',
            'ckpt': '/det/OpenPCDet/output/kitti_models/pillarfocus_ablation_no_dynamic/ablation_no_dynamic/ckpt/checkpoint_epoch_80.pth'
        },
        {
            'name': 'ablation_no_attention',
            'cfg': '/det/OpenPCDet/tools/cfgs/kitti_models/pillarfocus_ablation_no_attention.yaml',
            'ckpt': '/det/OpenPCDet/output/kitti_models/pillarfocus_ablation_no_attention/ablation_no_attention/ckpt/checkpoint_epoch_80.pth'
        }
    ]
    
    results = {}
    
    for model in models_to_eval:
        print(f"\n{'='*50}")
        print(f"Processing: {model['name']}")
        print(f"{'='*50}")
        
        result = run_evaluation(
            model['cfg'], 
            model['ckpt'], 
            model['name'], 
            results_dir
        )
        results[model['name']] = result
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    generate_comparison_report(results, results_dir)

def generate_comparison_report(results, output_dir):
    """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
    report_file = f"{output_dir}/comparison_report.md"
    
    with open(report_file, 'w') as f:
        f.write("# PillarFocusæ¨¡å‹è¯„ä¼°å¯¹æ¯”æŠ¥å‘Š\n\n")
        f.write("## æ¨¡å‹é…ç½®\n\n")
        f.write("| æ¨¡å‹ | æè¿° |\n")
        f.write("|------|------|\n")
        f.write("| baseline_pointpillar | æ ‡å‡†PointPillaråŸºå‡†æ¨¡å‹ |\n")
        f.write("| pillarfocus_full | å®Œæ•´PillarFocusæ¨¡å‹ |\n")
        f.write("| ablation_no_dynamic | æ— Dynamic Pillaræœºåˆ¶ |\n")
        f.write("| ablation_no_attention | æ— ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶ |\n\n")
        
        f.write("## è¯„ä¼°ç»“æœ\n\n")
        
        for model_name, result in results.items():
            f.write(f"### {model_name}\n\n")
            if result:
                f.write("```\n")
                f.write(result)
                f.write("\n```\n\n")
            else:
                f.write("âŒ è¯„ä¼°å¤±è´¥\n\n")
    
    print(f"ğŸ“Š Comparison report saved to: {report_file}")

if __name__ == "__main__":
    print("ğŸš€ Starting evaluation script...")
    main()
    print("âœ… Script completed!")